# 11.4 Cluster Elasticsearch simple (3 nÅ“uds)

ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

---

## ğŸ“‹ Introduction

Un **cluster Elasticsearch** est un ensemble de nÅ“uds (serveurs) qui travaillent ensemble pour stocker et gÃ©rer vos donnÃ©es. Contrairement Ã  un nÅ“ud unique, un cluster offre :

- ğŸ”„ **Haute disponibilitÃ©** : Si un nÅ“ud tombe, les autres continuent
- âš¡ **Performance** : Les requÃªtes sont distribuÃ©es sur plusieurs nÅ“uds
- ğŸ’¾ **RÃ©plication** : Les donnÃ©es sont dupliquÃ©es pour Ã©viter les pertes
- ğŸ“ˆ **ScalabilitÃ©** : Facile d'ajouter des nÅ“uds pour gÃ©rer plus de donnÃ©es

### ğŸ¯ Concepts clÃ©s

| Concept | Explication |
|---------|-------------|
| **Cluster** | Ensemble de nÅ“uds travaillant ensemble |
| **NÅ“ud** | Une instance Elasticsearch |
| **Shard** | Morceau d'un index distribuÃ© sur les nÅ“uds |
| **Replica** | Copie d'un shard pour la redondance |
| **Master node** | NÅ“ud qui gÃ¨re le cluster |

### ğŸ“¦ Ce que vous allez apprendre

Dans cette fiche, nous allons crÃ©er un **cluster de 3 nÅ“uds** Elasticsearch avec Docker Compose, parfait pour comprendre le clustering en environnement de dÃ©veloppement.

---

## âš ï¸ PrÃ©requis Importants

### Configuration systÃ¨me minimale

- âœ… **Docker** installÃ© (version 20.10+)
- âœ… **Docker Compose** installÃ© (version 2.0+)
- âœ… **Au moins 8 GB de RAM** disponibles
  - 2 GB par nÅ“ud Ã— 3 nÅ“uds = 6 GB minimum
  - + 2 GB pour le systÃ¨me
- âœ… **Espace disque** : Au moins 10 GB libres

> âš ï¸ **Attention** : Un cluster de 3 nÅ“uds consomme beaucoup de ressources. Assurez-vous d'avoir suffisamment de RAM avant de commencer.

### Configuration Linux (obligatoire)

Si vous Ãªtes sur **Linux**, configurez la mÃ©moire virtuelle :

```bash
# VÃ©rifier la valeur actuelle
sysctl vm.max_map_count

# Si < 262144, augmentez-la
sudo sysctl -w vm.max_map_count=262144

# Rendre le changement permanent
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
```

---

## ğŸš€ Configuration du Cluster 3 NÅ“uds

### Ã‰tape 1 : CrÃ©er le fichier `docker-compose.yml`

CrÃ©ez un nouveau dossier (par exemple `elasticsearch_cluster`) et placez-y ce fichier :

```yaml
version: '3.8'

services:
  # ===== NÅ’UD 1 : MASTER + DATA =====
  es-node01:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node01
    restart: unless-stopped

    environment:
      # Nom du nÅ“ud
      - node.name=es-node01

      # Nom du cluster (tous les nÅ“uds doivent avoir le mÃªme)
      - cluster.name=es-docker-cluster

      # Liste des nÅ“uds Ã©ligibles comme master
      # Important : liste les 3 nÅ“uds pour l'Ã©lection du master
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03

      # Liste de tous les nÅ“uds du cluster (pour la dÃ©couverte)
      - discovery.seed_hosts=es-node02,es-node03

      # DÃ©sactive la sÃ©curitÃ© (dÃ©veloppement uniquement)
      - xpack.security.enabled=false

      # MÃ©moire : 1 GB min, 2 GB max par nÅ“ud
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"

      # Interface d'Ã©coute
      - network.host=0.0.0.0

    ports:
      # API REST du nÅ“ud 1
      - "9201:9200"
      # Port de communication inter-nÅ“uds
      - "9301:9300"

    volumes:
      # DonnÃ©es du nÅ“ud 1
      - es-data01:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== NÅ’UD 2 : MASTER + DATA =====
  es-node02:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node02
    restart: unless-stopped

    environment:
      - node.name=es-node02
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03
      - discovery.seed_hosts=es-node01,es-node03
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"
      - network.host=0.0.0.0

    ports:
      # API REST du nÅ“ud 2 (port diffÃ©rent pour accÃ¨s externe)
      - "9202:9200"
      - "9302:9300"

    volumes:
      - es-data02:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== NÅ’UD 3 : MASTER + DATA =====
  es-node03:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node03
    restart: unless-stopped

    environment:
      - node.name=es-node03
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03
      - discovery.seed_hosts=es-node01,es-node02
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"
      - network.host=0.0.0.0

    ports:
      # API REST du nÅ“ud 3
      - "9203:9200"
      - "9303:9300"

    volumes:
      - es-data03:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

# Volumes (un par nÅ“ud)
volumes:
  es-data01:
    driver: local
  es-data02:
    driver: local
  es-data03:
    driver: local

# RÃ©seau partagÃ©
networks:
  elastic:
    driver: bridge
```

### ğŸ“ Explication des paramÃ¨tres clÃ©s

#### ParamÃ¨tres de clustering

| ParamÃ¨tre | Explication |
|-----------|-------------|
| `cluster.name` | Nom du cluster (doit Ãªtre identique sur tous les nÅ“uds) |
| `node.name` | Nom unique de chaque nÅ“ud |
| `cluster.initial_master_nodes` | Liste des nÅ“uds Ã©ligibles pour devenir master |
| `discovery.seed_hosts` | Liste des autres nÅ“uds pour la dÃ©couverte |

#### Ports exposÃ©s

| NÅ“ud | Port API REST | Port Communication |
|------|---------------|-------------------|
| es-node01 | 9201 â†’ 9200 | 9301 â†’ 9300 |
| es-node02 | 9202 â†’ 9200 | 9302 â†’ 9300 |
| es-node03 | 9203 â†’ 9200 | 9303 â†’ 9300 |

> ğŸ’¡ **Pourquoi des ports diffÃ©rents ?** Chaque nÅ“ud Ã©coute sur le port 9200 *Ã  l'intÃ©rieur* du conteneur, mais Docker les expose sur des ports diffÃ©rents (9201, 9202, 9203) pour Ã©viter les conflits.

---

## â–¶ï¸ Ã‰tape 2 : Lancer le Cluster

Dans le dossier contenant votre `docker-compose.yml` :

```bash
# DÃ©marrer les 3 nÅ“uds
docker-compose up -d

# Voir les logs de tous les nÅ“uds
docker-compose logs -f
```

### Ce qui se passe :

1. â³ **Les 3 nÅ“uds dÃ©marrent** (chacun prend 30-60 secondes)
2. ğŸ” **DÃ©couverte** : Les nÅ“uds se trouvent mutuellement
3. ğŸ—³ï¸ **Ã‰lection du master** : Un nÅ“ud est Ã©lu comme master
4. âœ… **Cluster formÃ©** : Les 3 nÅ“uds sont connectÃ©s

â±ï¸ **Temps total** : environ 2-3 minutes pour que le cluster soit complÃ¨tement opÃ©rationnel.

### Logs importants Ã  surveiller

Vous verrez dans les logs :

```
[es-node01] master node changed
[es-node02] added {es-node01} to cluster
[es-node03] added {es-node02} to cluster
cluster health status changed from [YELLOW] to [GREEN]
```

âœ… Quand vous voyez `status changed to [GREEN]`, le cluster est prÃªt !

---

## âœ… Ã‰tape 3 : VÃ©rifier le Cluster

### Test 1 : VÃ©rifier l'Ã©tat du cluster

Vous pouvez interroger **n'importe quel nÅ“ud** :

```bash
# Via le nÅ“ud 1
curl http://localhost:9201/_cluster/health?pretty

# Via le nÅ“ud 2
curl http://localhost:9202/_cluster/health?pretty

# Via le nÅ“ud 3
curl http://localhost:9203/_cluster/health?pretty
```

**RÃ©ponse attendue** :

```json
{
  "cluster_name" : "es-docker-cluster",
  "status" : "green",
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}
```

| Status | Signification |
|--------|---------------|
| ğŸŸ¢ `green` | Tout va bien, tous les shards sont assignÃ©s |
| ğŸŸ¡ `yellow` | Fonctionnel mais des replicas manquent |
| ğŸ”´ `red` | ProblÃ¨me, des donnÃ©es primaires manquent |

### Test 2 : Lister les nÅ“uds du cluster

```bash
curl http://localhost:9201/_cat/nodes?v
```

**RÃ©sultat attendu** :

```
ip         heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
172.18.0.2           45          78   3    0.12    0.23     0.45 cdfhilmrstw *      es-node01
172.18.0.3           42          78   2    0.12    0.23     0.45 cdfhilmrstw -      es-node02
172.18.0.4           40          78   1    0.12    0.23     0.45 cdfhilmrstw -      es-node03
```

âœ… Vous devriez voir **3 lignes** (un nÅ“ud par ligne).

L'**astÃ©risque (*)** indique quel nÅ“ud est le **master** actuel.

### Test 3 : Informations dÃ©taillÃ©es du cluster

```bash
curl http://localhost:9201/_cluster/stats?human&pretty
```

Vous verrez des statistiques complÃ¨tes sur le cluster.

---

## ğŸ§ª Tester la RÃ©plication

### CrÃ©er un index avec des replicas

```bash
# CrÃ©er un index avec 1 shard primaire et 2 replicas
curl -X PUT "http://localhost:9201/test-cluster?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "settings": {
         "number_of_shards": 1,
         "number_of_replicas": 2
       }
     }'
```

**Explication** :

- `number_of_shards: 1` : Un seul shard primaire
- `number_of_replicas: 2` : Deux copies (une sur chaque autre nÅ“ud)

### VÃ©rifier la distribution des shards

```bash
curl http://localhost:9201/_cat/shards/test-cluster?v
```

**RÃ©sultat attendu** :

```
index        shard prirep state   docs store ip         node
test-cluster 0     p      STARTED    0  208b 172.18.0.2 es-node01
test-cluster 0     r      STARTED    0  208b 172.18.0.3 es-node02
test-cluster 0     r      STARTED    0  208b 172.18.0.4 es-node03
```

âœ… Vous voyez :
- **p** (primary) : Le shard primaire sur es-node01
- **r** (replica) : Les replicas sur es-node02 et es-node03

### Ajouter des donnÃ©es

```bash
# Ajouter un document via le nÅ“ud 1
curl -X POST "http://localhost:9201/test-cluster/_doc?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "message": "Document distribuÃ© sur le cluster",
       "timestamp": "2025-11-01T10:00:00"
     }'
```

### VÃ©rifier que les donnÃ©es sont disponibles sur tous les nÅ“uds

```bash
# Recherche via le nÅ“ud 1
curl http://localhost:9201/test-cluster/_search?pretty

# Recherche via le nÅ“ud 2 (mÃªme rÃ©sultat !)
curl http://localhost:9202/test-cluster/_search?pretty

# Recherche via le nÅ“ud 3 (mÃªme rÃ©sultat !)
curl http://localhost:9203/test-cluster/_search?pretty
```

âœ… Vous verrez le **mÃªme document** depuis n'importe quel nÅ“ud !

---

## ğŸ”„ Simuler une Panne de NÅ“ud

### Test de haute disponibilitÃ©

```bash
# ArrÃªter le nÅ“ud 2
docker-compose stop es-node02

# VÃ©rifier l'Ã©tat du cluster
curl http://localhost:9201/_cluster/health?pretty
```

**Ce qui se passe** :

1. âš ï¸ Le cluster passe en **YELLOW** (replica manquant)
2. ğŸ”„ Elasticsearch redistribue les shards
3. âœ… Les donnÃ©es restent **accessibles** via les nÅ“uds 1 et 3

### VÃ©rifier que les donnÃ©es sont toujours lÃ 

```bash
# Via le nÅ“ud 1 (toujours actif)
curl http://localhost:9201/test-cluster/_search?pretty

# Via le nÅ“ud 3 (toujours actif)
curl http://localhost:9203/test-cluster/_search?pretty
```

âœ… Les donnÃ©es sont **toujours disponibles** !

### RedÃ©marrer le nÅ“ud

```bash
# RedÃ©marrer le nÅ“ud 2
docker-compose start es-node02

# Attendre 30 secondes, puis vÃ©rifier
curl http://localhost:9201/_cluster/health?pretty
```

âœ… Le cluster repasse en **GREEN** !

---

## ğŸ”§ Commandes Utiles

### Gestion du cluster

```bash
# Voir l'Ã©tat de tous les nÅ“uds
docker-compose ps

# Logs de tous les nÅ“uds
docker-compose logs -f

# Logs d'un nÅ“ud spÃ©cifique
docker-compose logs -f es-node01

# RedÃ©marrer un nÅ“ud
docker-compose restart es-node02

# ArrÃªter tout le cluster
docker-compose stop

# RedÃ©marrer tout le cluster
docker-compose start
```

### Monitoring du cluster

```bash
# Ã‰tat gÃ©nÃ©ral
curl http://localhost:9201/_cluster/health?pretty

# Liste des nÅ“uds
curl http://localhost:9201/_cat/nodes?v

# Liste des index
curl http://localhost:9201/_cat/indices?v

# Distribution des shards
curl http://localhost:9201/_cat/shards?v

# Allocation des shards (dÃ©tails)
curl http://localhost:9201/_cat/allocation?v

# Stats du cluster
curl http://localhost:9201/_cluster/stats?human&pretty
```

---

## ğŸ“Š Ajouter Kibana au Cluster

Vous pouvez ajouter Kibana pour visualiser votre cluster.

**Ajoutez ce service dans votre `docker-compose.yml`** :

```yaml
  # ===== KIBANA =====
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana_cluster
    restart: unless-stopped

    environment:
      # Se connecte Ã  n'importe quel nÅ“ud (ici le nÅ“ud 1)
      - ELASTICSEARCH_HOSTS=http://es-node01:9200
      - xpack.security.enabled=false
      - SERVER_HOST=0.0.0.0

    ports:
      - "5601:5601"

    networks:
      - elastic

    depends_on:
      es-node01:
        condition: service_healthy
```

Puis relancez :

```bash
docker-compose up -d kibana
```

AccÃ©dez Ã  Kibana : `http://localhost:5601`

Dans **Stack Monitoring**, vous pourrez voir tous vos nÅ“uds visuellement ! ğŸ“Š

---

## ğŸ›‘ Suppression ComplÃ¨te

Pour tout supprimer :

```bash
# 1. ArrÃªter et supprimer tous les conteneurs
docker-compose down

# 2. Supprimer tous les volumes (âš ï¸ IRRÃ‰VERSIBLE)
docker volume rm \
  <nom_du_dossier>_es-data01 \
  <nom_du_dossier>_es-data02 \
  <nom_du_dossier>_es-data03

# Pour trouver les noms exacts :
docker volume ls | grep es-data
```

---

## ğŸ› ProblÃ¨mes Courants

### 1. Le cluster reste en YELLOW

**Cause** : Pas assez de nÅ“uds pour tous les replicas.

**Exemple** : Index avec 3 replicas mais seulement 3 nÅ“uds (impossible de placer toutes les copies).

**Solution** : RÃ©duire le nombre de replicas :

```bash
curl -X PUT "http://localhost:9201/mon-index/_settings?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "number_of_replicas": 2
     }'
```

### 2. Les nÅ“uds ne se trouvent pas

**SymptÃ´me** : `number_of_nodes` reste Ã  1.

**Solutions** :

```bash
# VÃ©rifier que tous les conteneurs tournent
docker-compose ps

# VÃ©rifier les logs
docker-compose logs | grep "discovery"

# VÃ©rifier qu'ils sont sur le mÃªme rÃ©seau
docker network inspect <nom_du_dossier>_elastic
```

### 3. "master not discovered yet"

**Cause** : Les nÅ“uds ne peuvent pas Ã©lire un master.

**Solution** : VÃ©rifier la configuration `cluster.initial_master_nodes` (doit Ãªtre identique sur tous les nÅ“uds).

### 4. Manque de mÃ©moire

**SymptÃ´me** : Les conteneurs redÃ©marrent en boucle.

**Solution** : RÃ©duire la mÃ©moire par nÅ“ud :

```yaml
- "ES_JAVA_OPTS=-Xms512m -Xmx1g"
```

### 5. "max virtual memory areas too low" (Linux)

**Solution** :

```bash
sudo sysctl -w vm.max_map_count=262144
```

---

## ğŸ“Š Tableau RÃ©capitulatif

### Configuration du cluster

| Ã‰lÃ©ment | Valeur |
|---------|--------|
| **Nombre de nÅ“uds** | 3 |
| **Nom du cluster** | es-docker-cluster |
| **NÅ“uds master** | Les 3 nÅ“uds |
| **RÃ©plication** | Possible (jusqu'Ã  2 replicas) |
| **RAM totale** | ~6 GB (2 GB par nÅ“ud) |

### Ports d'accÃ¨s

| NÅ“ud | URL API REST |
|------|--------------|
| es-node01 | `http://localhost:9201` |
| es-node02 | `http://localhost:9202` |
| es-node03 | `http://localhost:9203` |

---

## ğŸ’¡ Bonnes Pratiques

### Pour le dÃ©veloppement

âœ… **3 nÅ“uds minimum** : Pour tester la haute disponibilitÃ©
âœ… **Noms explicites** : `es-node01`, `es-node02`, etc.
âœ… **Volumes sÃ©parÃ©s** : Un volume par nÅ“ud
âœ… **Surveillez la RAM** : 8 GB minimum recommandÃ©s

### Pour la production

ğŸ”’ **Activez la sÃ©curitÃ©** : Authentification obligatoire
ğŸ” **Utilisez HTTPS** : Chiffrement des communications
ğŸ¯ **RÃ´les dÃ©diÃ©s** : NÅ“uds master / data / coordination sÃ©parÃ©s
ğŸŒ **Plusieurs zones** : Pour la rÃ©silience gÃ©ographique
ğŸ“Š **Monitoring** : Kibana Stack Monitoring ou Prometheus
ğŸ’¾ **Snapshots** : Backups rÃ©guliers

---

## ğŸ” Comprendre les RÃ´les de NÅ“uds

Dans notre configuration, tous les nÅ“uds ont **tous les rÃ´les** (master + data). En production, on sÃ©pare souvent :

| RÃ´le | Fonction |
|------|----------|
| **Master** | GÃ¨re le cluster (Ã©lections, crÃ©ation d'index, etc.) |
| **Data** | Stocke les donnÃ©es et exÃ©cute les requÃªtes |
| **Ingest** | Traite les donnÃ©es avant indexation |
| **Coordination** | Distribue les requÃªtes (pas de donnÃ©es) |

### Configuration avec rÃ´les sÃ©parÃ©s (avancÃ©)

```yaml
# NÅ“ud master uniquement
- node.roles=["master"]

# NÅ“ud data uniquement
- node.roles=["data"]

# NÅ“ud coordination uniquement
- node.roles=[]
```

---

## ğŸ“š Ressources ComplÃ©mentaires

- ğŸ“– [Elasticsearch Cluster Architecture](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html)
- ğŸ“ [Clustering and Node Discovery](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html)
- ğŸ” [Shard Allocation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html)
- ğŸ“Š [Cluster Health](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html)

---

## ğŸ¯ Prochaines Ã‰tapes

Maintenant que vous avez un cluster Elasticsearch, vous pouvez :

- ğŸ“Š Explorer les [cas pratiques - Stack ELK](/cas-pratiques/03-stack-elk.md)
- ğŸ–¥ï¸ Retour Ã  [Elasticsearch avec Kibana](03-elasticsearch-kibana.md)
- ğŸŒ Voir la [configuration avec IP fixe](02-config-ip-fixe.md)
- ğŸ  Retour Ã  la [configuration basique](01-config-noeud-unique.md)

---

## ğŸ’¡ Points ClÃ©s Ã  Retenir

âœ… Un **cluster** amÃ©liore la disponibilitÃ© et les performances
âœ… **3 nÅ“uds minimum** pour une vraie haute disponibilitÃ©
âœ… Les **replicas** protÃ¨gent contre la perte de donnÃ©es
âœ… Un cluster peut fonctionner mÃªme si **1 nÅ“ud tombe**
âœ… Tous les nÅ“uds peuvent recevoir des **requÃªtes API**
âœ… Le **master node** est Ã©lu automatiquement
âœ… **8 GB de RAM minimum** pour un cluster de 3 nÅ“uds
âœ… Le statut **GREEN** signifie que tout va bien

---

ğŸ” Retour au [Sommaire](/SOMMAIRE.md)


