# 11.4 Cluster Elasticsearch simple (3 n≈ìuds)

üîù Retour au [Sommaire](/SOMMAIRE.md)

---

## üìã Introduction

Un **cluster Elasticsearch** est un ensemble de n≈ìuds (serveurs) qui travaillent ensemble pour stocker et g√©rer vos donn√©es. Contrairement √† un n≈ìud unique, un cluster offre :

- üîÑ **Haute disponibilit√©** : Si un n≈ìud tombe, les autres continuent
- ‚ö° **Performance** : Les requ√™tes sont distribu√©es sur plusieurs n≈ìuds
- üíæ **R√©plication** : Les donn√©es sont dupliqu√©es pour √©viter les pertes
- üìà **Scalabilit√©** : Facile d'ajouter des n≈ìuds pour g√©rer plus de donn√©es

### üéØ Concepts cl√©s

| Concept | Explication |
|---------|-------------|
| **Cluster** | Ensemble de n≈ìuds travaillant ensemble |
| **N≈ìud** | Une instance Elasticsearch |
| **Shard** | Morceau d'un index distribu√© sur les n≈ìuds |
| **Replica** | Copie d'un shard pour la redondance |
| **Master node** | N≈ìud qui g√®re le cluster |

### üì¶ Ce que vous allez apprendre

Dans cette fiche, nous allons cr√©er un **cluster de 3 n≈ìuds** Elasticsearch avec Docker Compose, parfait pour comprendre le clustering en environnement de d√©veloppement.

---

## ‚ö†Ô∏è Pr√©requis Importants

### Configuration syst√®me minimale

- ‚úÖ **Docker** install√© (version 20.10+)
- ‚úÖ **Docker Compose** install√© (version 2.0+)
- ‚úÖ **Au moins 8 GB de RAM** disponibles
  - 2 GB par n≈ìud √ó 3 n≈ìuds = 6 GB minimum
  - + 2 GB pour le syst√®me
- ‚úÖ **Espace disque** : Au moins 10 GB libres

> ‚ö†Ô∏è **Attention** : Un cluster de 3 n≈ìuds consomme beaucoup de ressources. Assurez-vous d'avoir suffisamment de RAM avant de commencer.

### Configuration Linux (obligatoire)

Si vous √™tes sur **Linux**, configurez la m√©moire virtuelle :

```bash
# V√©rifier la valeur actuelle
sysctl vm.max_map_count

# Si < 262144, augmentez-la
sudo sysctl -w vm.max_map_count=262144

# Rendre le changement permanent
echo "vm.max_map_count=262144" | sudo tee -a /etc/sysctl.conf
```

---

## üöÄ Configuration du Cluster 3 N≈ìuds

### √âtape 1 : Cr√©er le fichier `docker-compose.yml`

Cr√©ez un nouveau dossier (par exemple `elasticsearch_cluster`) et placez-y ce fichier :

```yaml
version: '3.8'

services:
  # ===== N≈íUD 1 : MASTER + DATA =====
  es-node01:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node01
    restart: unless-stopped

    environment:
      # Nom du n≈ìud
      - node.name=es-node01

      # Nom du cluster (tous les n≈ìuds doivent avoir le m√™me)
      - cluster.name=es-docker-cluster

      # Liste des n≈ìuds √©ligibles comme master
      # Important : liste les 3 n≈ìuds pour l'√©lection du master
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03

      # Liste de tous les n≈ìuds du cluster (pour la d√©couverte)
      - discovery.seed_hosts=es-node02,es-node03

      # D√©sactive la s√©curit√© (d√©veloppement uniquement)
      - xpack.security.enabled=false

      # M√©moire : 1 GB min, 2 GB max par n≈ìud
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"

      # Interface d'√©coute
      - network.host=0.0.0.0

    ports:
      # API REST du n≈ìud 1
      - "9201:9200"
      # Port de communication inter-n≈ìuds
      - "9301:9300"

    volumes:
      # Donn√©es du n≈ìud 1
      - es-data01:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== N≈íUD 2 : MASTER + DATA =====
  es-node02:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node02
    restart: unless-stopped

    environment:
      - node.name=es-node02
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03
      - discovery.seed_hosts=es-node01,es-node03
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"
      - network.host=0.0.0.0

    ports:
      # API REST du n≈ìud 2 (port diff√©rent pour acc√®s externe)
      - "9202:9200"
      - "9302:9300"

    volumes:
      - es-data02:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

  # ===== N≈íUD 3 : MASTER + DATA =====
  es-node03:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: es-node03
    restart: unless-stopped

    environment:
      - node.name=es-node03
      - cluster.name=es-docker-cluster
      - cluster.initial_master_nodes=es-node01,es-node02,es-node03
      - discovery.seed_hosts=es-node01,es-node02
      - xpack.security.enabled=false
      - "ES_JAVA_OPTS=-Xms1g -Xmx2g"
      - network.host=0.0.0.0

    ports:
      # API REST du n≈ìud 3
      - "9203:9200"
      - "9303:9300"

    volumes:
      - es-data03:/usr/share/elasticsearch/data

    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536

    networks:
      - elastic

    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9200/_cluster/health || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5

# Volumes (un par n≈ìud)
volumes:
  es-data01:
    driver: local
  es-data02:
    driver: local
  es-data03:
    driver: local

# R√©seau partag√©
networks:
  elastic:
    driver: bridge
```

### üìù Explication des param√®tres cl√©s

#### Param√®tres de clustering

| Param√®tre | Explication |
|-----------|-------------|
| `cluster.name` | Nom du cluster (doit √™tre identique sur tous les n≈ìuds) |
| `node.name` | Nom unique de chaque n≈ìud |
| `cluster.initial_master_nodes` | Liste des n≈ìuds √©ligibles pour devenir master |
| `discovery.seed_hosts` | Liste des autres n≈ìuds pour la d√©couverte |

#### Ports expos√©s

| N≈ìud | Port API REST | Port Communication |
|------|---------------|-------------------|
| es-node01 | 9201 ‚Üí 9200 | 9301 ‚Üí 9300 |
| es-node02 | 9202 ‚Üí 9200 | 9302 ‚Üí 9300 |
| es-node03 | 9203 ‚Üí 9200 | 9303 ‚Üí 9300 |

> üí° **Pourquoi des ports diff√©rents ?** Chaque n≈ìud √©coute sur le port 9200 *√† l'int√©rieur* du conteneur, mais Docker les expose sur des ports diff√©rents (9201, 9202, 9203) pour √©viter les conflits.

---

## ‚ñ∂Ô∏è √âtape 2 : Lancer le Cluster

Dans le dossier contenant votre `docker-compose.yml` :

```bash
# D√©marrer les 3 n≈ìuds
docker-compose up -d

# Voir les logs de tous les n≈ìuds
docker-compose logs -f
```

### Ce qui se passe :

1. ‚è≥ **Les 3 n≈ìuds d√©marrent** (chacun prend 30-60 secondes)
2. üîç **D√©couverte** : Les n≈ìuds se trouvent mutuellement
3. üó≥Ô∏è **√âlection du master** : Un n≈ìud est √©lu comme master
4. ‚úÖ **Cluster form√©** : Les 3 n≈ìuds sont connect√©s

‚è±Ô∏è **Temps total** : environ 2-3 minutes pour que le cluster soit compl√®tement op√©rationnel.

### Logs importants √† surveiller

Vous verrez dans les logs :

```
[es-node01] master node changed
[es-node02] added {es-node01} to cluster
[es-node03] added {es-node02} to cluster
cluster health status changed from [YELLOW] to [GREEN]
```

‚úÖ Quand vous voyez `status changed to [GREEN]`, le cluster est pr√™t !

---

## ‚úÖ √âtape 3 : V√©rifier le Cluster

### Test 1 : V√©rifier l'√©tat du cluster

Vous pouvez interroger **n'importe quel n≈ìud** :

```bash
# Via le n≈ìud 1
curl http://localhost:9201/_cluster/health?pretty

# Via le n≈ìud 2
curl http://localhost:9202/_cluster/health?pretty

# Via le n≈ìud 3
curl http://localhost:9203/_cluster/health?pretty
```

**R√©ponse attendue** :

```json
{
  "cluster_name" : "es-docker-cluster",
  "status" : "green",
  "number_of_nodes" : 3,
  "number_of_data_nodes" : 3,
  "active_primary_shards" : 0,
  "active_shards" : 0,
  "relocating_shards" : 0,
  "initializing_shards" : 0,
  "unassigned_shards" : 0
}
```

| Status | Signification |
|--------|---------------|
| üü¢ `green` | Tout va bien, tous les shards sont assign√©s |
| üü° `yellow` | Fonctionnel mais des replicas manquent |
| üî¥ `red` | Probl√®me, des donn√©es primaires manquent |

### Test 2 : Lister les n≈ìuds du cluster

```bash
curl http://localhost:9201/_cat/nodes?v
```

**R√©sultat attendu** :

```
ip         heap.percent ram.percent cpu load_1m load_5m load_15m node.role master name
172.18.0.2           45          78   3    0.12    0.23     0.45 cdfhilmrstw *      es-node01
172.18.0.3           42          78   2    0.12    0.23     0.45 cdfhilmrstw -      es-node02
172.18.0.4           40          78   1    0.12    0.23     0.45 cdfhilmrstw -      es-node03
```

‚úÖ Vous devriez voir **3 lignes** (un n≈ìud par ligne).

L'**ast√©risque (*)** indique quel n≈ìud est le **master** actuel.

### Test 3 : Informations d√©taill√©es du cluster

```bash
curl http://localhost:9201/_cluster/stats?human&pretty
```

Vous verrez des statistiques compl√®tes sur le cluster.

---

## üß™ Tester la R√©plication

### Cr√©er un index avec des replicas

```bash
# Cr√©er un index avec 1 shard primaire et 2 replicas
curl -X PUT "http://localhost:9201/test-cluster?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "settings": {
         "number_of_shards": 1,
         "number_of_replicas": 2
       }
     }'
```

**Explication** :

- `number_of_shards: 1` : Un seul shard primaire
- `number_of_replicas: 2` : Deux copies (une sur chaque autre n≈ìud)

### V√©rifier la distribution des shards

```bash
curl http://localhost:9201/_cat/shards/test-cluster?v
```

**R√©sultat attendu** :

```
index        shard prirep state   docs store ip         node
test-cluster 0     p      STARTED    0  208b 172.18.0.2 es-node01
test-cluster 0     r      STARTED    0  208b 172.18.0.3 es-node02
test-cluster 0     r      STARTED    0  208b 172.18.0.4 es-node03
```

‚úÖ Vous voyez :
- **p** (primary) : Le shard primaire sur es-node01
- **r** (replica) : Les replicas sur es-node02 et es-node03

### Ajouter des donn√©es

```bash
# Ajouter un document via le n≈ìud 1
curl -X POST "http://localhost:9201/test-cluster/_doc?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "message": "Document distribu√© sur le cluster",
       "timestamp": "2025-11-01T10:00:00"
     }'
```

### V√©rifier que les donn√©es sont disponibles sur tous les n≈ìuds

```bash
# Recherche via le n≈ìud 1
curl http://localhost:9201/test-cluster/_search?pretty

# Recherche via le n≈ìud 2 (m√™me r√©sultat !)
curl http://localhost:9202/test-cluster/_search?pretty

# Recherche via le n≈ìud 3 (m√™me r√©sultat !)
curl http://localhost:9203/test-cluster/_search?pretty
```

‚úÖ Vous verrez le **m√™me document** depuis n'importe quel n≈ìud !

---

## üîÑ Simuler une Panne de N≈ìud

### Test de haute disponibilit√©

```bash
# Arr√™ter le n≈ìud 2
docker-compose stop es-node02

# V√©rifier l'√©tat du cluster
curl http://localhost:9201/_cluster/health?pretty
```

**Ce qui se passe** :

1. ‚ö†Ô∏è Le cluster passe en **YELLOW** (replica manquant)
2. üîÑ Elasticsearch redistribue les shards
3. ‚úÖ Les donn√©es restent **accessibles** via les n≈ìuds 1 et 3

### V√©rifier que les donn√©es sont toujours l√†

```bash
# Via le n≈ìud 1 (toujours actif)
curl http://localhost:9201/test-cluster/_search?pretty

# Via le n≈ìud 3 (toujours actif)
curl http://localhost:9203/test-cluster/_search?pretty
```

‚úÖ Les donn√©es sont **toujours disponibles** !

### Red√©marrer le n≈ìud

```bash
# Red√©marrer le n≈ìud 2
docker-compose start es-node02

# Attendre 30 secondes, puis v√©rifier
curl http://localhost:9201/_cluster/health?pretty
```

‚úÖ Le cluster repasse en **GREEN** !

---

## üîß Commandes Utiles

### Gestion du cluster

```bash
# Voir l'√©tat de tous les n≈ìuds
docker-compose ps

# Logs de tous les n≈ìuds
docker-compose logs -f

# Logs d'un n≈ìud sp√©cifique
docker-compose logs -f es-node01

# Red√©marrer un n≈ìud
docker-compose restart es-node02

# Arr√™ter tout le cluster
docker-compose stop

# Red√©marrer tout le cluster
docker-compose start
```

### Monitoring du cluster

```bash
# √âtat g√©n√©ral
curl http://localhost:9201/_cluster/health?pretty

# Liste des n≈ìuds
curl http://localhost:9201/_cat/nodes?v

# Liste des index
curl http://localhost:9201/_cat/indices?v

# Distribution des shards
curl http://localhost:9201/_cat/shards?v

# Allocation des shards (d√©tails)
curl http://localhost:9201/_cat/allocation?v

# Stats du cluster
curl http://localhost:9201/_cluster/stats?human&pretty
```

---

## üìä Ajouter Kibana au Cluster

Vous pouvez ajouter Kibana pour visualiser votre cluster.

**Ajoutez ce service dans votre `docker-compose.yml`** :

```yaml
  # ===== KIBANA =====
  kibana:
    image: docker.elastic.co/kibana/kibana:8.11.0
    container_name: kibana_cluster
    restart: unless-stopped

    environment:
      # Se connecte √† n'importe quel n≈ìud (ici le n≈ìud 1)
      - ELASTICSEARCH_HOSTS=http://es-node01:9200
      - xpack.security.enabled=false
      - SERVER_HOST=0.0.0.0

    ports:
      - "5601:5601"

    networks:
      - elastic

    depends_on:
      es-node01:
        condition: service_healthy
```

Puis relancez :

```bash
docker-compose up -d kibana
```

Acc√©dez √† Kibana : `http://localhost:5601`

Dans **Stack Monitoring**, vous pourrez voir tous vos n≈ìuds visuellement ! üìä

---

## üõë Suppression Compl√®te

Pour tout supprimer :

```bash
# 1. Arr√™ter et supprimer tous les conteneurs
docker-compose down

# 2. Supprimer tous les volumes (‚ö†Ô∏è IRR√âVERSIBLE)
docker volume rm \
  <nom_du_dossier>_es-data01 \
  <nom_du_dossier>_es-data02 \
  <nom_du_dossier>_es-data03

# Pour trouver les noms exacts :
docker volume ls | grep es-data
```

---

## üêõ Probl√®mes Courants

### 1. Le cluster reste en YELLOW

**Cause** : Pas assez de n≈ìuds pour tous les replicas.

**Exemple** : Index avec 3 replicas mais seulement 3 n≈ìuds (impossible de placer toutes les copies).

**Solution** : R√©duire le nombre de replicas :

```bash
curl -X PUT "http://localhost:9201/mon-index/_settings?pretty" \
     -H 'Content-Type: application/json' \
     -d '{
       "number_of_replicas": 2
     }'
```

### 2. Les n≈ìuds ne se trouvent pas

**Sympt√¥me** : `number_of_nodes` reste √† 1.

**Solutions** :

```bash
# V√©rifier que tous les conteneurs tournent
docker-compose ps

# V√©rifier les logs
docker-compose logs | grep "discovery"

# V√©rifier qu'ils sont sur le m√™me r√©seau
docker network inspect <nom_du_dossier>_elastic
```

### 3. "master not discovered yet"

**Cause** : Les n≈ìuds ne peuvent pas √©lire un master.

**Solution** : V√©rifier la configuration `cluster.initial_master_nodes` (doit √™tre identique sur tous les n≈ìuds).

### 4. Manque de m√©moire

**Sympt√¥me** : Les conteneurs red√©marrent en boucle.

**Solution** : R√©duire la m√©moire par n≈ìud :

```yaml
- "ES_JAVA_OPTS=-Xms512m -Xmx1g"
```

### 5. "max virtual memory areas too low" (Linux)

**Solution** :

```bash
sudo sysctl -w vm.max_map_count=262144
```

---

## üìä Tableau R√©capitulatif

### Configuration du cluster

| √âl√©ment | Valeur |
|---------|--------|
| **Nombre de n≈ìuds** | 3 |
| **Nom du cluster** | es-docker-cluster |
| **N≈ìuds master** | Les 3 n≈ìuds |
| **R√©plication** | Possible (jusqu'√† 2 replicas) |
| **RAM totale** | ~6 GB (2 GB par n≈ìud) |

### Ports d'acc√®s

| N≈ìud | URL API REST |
|------|--------------|
| es-node01 | `http://localhost:9201` |
| es-node02 | `http://localhost:9202` |
| es-node03 | `http://localhost:9203` |

---

## üí° Bonnes Pratiques

### Pour le d√©veloppement

‚úÖ **3 n≈ìuds minimum** : Pour tester la haute disponibilit√©
‚úÖ **Noms explicites** : `es-node01`, `es-node02`, etc.
‚úÖ **Volumes s√©par√©s** : Un volume par n≈ìud
‚úÖ **Surveillez la RAM** : 8 GB minimum recommand√©s

### Pour la production

üîí **Activez la s√©curit√©** : Authentification obligatoire
üîê **Utilisez HTTPS** : Chiffrement des communications
üéØ **R√¥les d√©di√©s** : N≈ìuds master / data / coordination s√©par√©s
üåê **Plusieurs zones** : Pour la r√©silience g√©ographique
üìä **Monitoring** : Kibana Stack Monitoring ou Prometheus
üíæ **Snapshots** : Backups r√©guliers

---

## üîç Comprendre les R√¥les de N≈ìuds

Dans notre configuration, tous les n≈ìuds ont **tous les r√¥les** (master + data). En production, on s√©pare souvent :

| R√¥le | Fonction |
|------|----------|
| **Master** | G√®re le cluster (√©lections, cr√©ation d'index, etc.) |
| **Data** | Stocke les donn√©es et ex√©cute les requ√™tes |
| **Ingest** | Traite les donn√©es avant indexation |
| **Coordination** | Distribue les requ√™tes (pas de donn√©es) |

### Configuration avec r√¥les s√©par√©s (avanc√©)

```yaml
# N≈ìud master uniquement
- node.roles=["master"]

# N≈ìud data uniquement
- node.roles=["data"]

# N≈ìud coordination uniquement
- node.roles=[]
```

---

## üìö Ressources Compl√©mentaires

- üìñ [Elasticsearch Cluster Architecture](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-node.html)
- üéì [Clustering and Node Discovery](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-discovery.html)
- üîç [Shard Allocation](https://www.elastic.co/guide/en/elasticsearch/reference/current/modules-cluster.html)
- üìä [Cluster Health](https://www.elastic.co/guide/en/elasticsearch/reference/current/cluster-health.html)

---

## üéØ Prochaines √âtapes

Maintenant que vous avez un cluster Elasticsearch, vous pouvez :

- üìä Explorer les [cas pratiques - Stack ELK](/cas-pratiques/03-stack-elk.md)
- üñ•Ô∏è Retour √† [Elasticsearch avec Kibana](03-elasticsearch-kibana.md)
- üåê Voir la [configuration avec IP fixe](02-config-ip-fixe.md)
- üè† Retour √† la [configuration basique](01-config-noeud-unique.md)

---

## üí° Points Cl√©s √† Retenir

‚úÖ Un **cluster** am√©liore la disponibilit√© et les performances
‚úÖ **3 n≈ìuds minimum** pour une vraie haute disponibilit√©
‚úÖ Les **replicas** prot√®gent contre la perte de donn√©es
‚úÖ Un cluster peut fonctionner m√™me si **1 n≈ìud tombe**
‚úÖ Tous les n≈ìuds peuvent recevoir des **requ√™tes API**
‚úÖ Le **master node** est √©lu automatiquement
‚úÖ **8 GB de RAM minimum** pour un cluster de 3 n≈ìuds
‚úÖ Le statut **GREEN** signifie que tout va bien

---

üîù Retour au [Sommaire](/SOMMAIRE.md)


