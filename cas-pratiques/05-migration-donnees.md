# Migration de DonnÃ©es entre Bases de DonnÃ©es

ğŸ” Retour au [Sommaire](/SOMMAIRE.md)

---

## ğŸ“‹ Introduction

La **migration de donnÃ©es** consiste Ã  transfÃ©rer des donnÃ©es d'un systÃ¨me de gestion de base de donnÃ©es (SGBD) vers un autre. C'est une opÃ©ration courante dans le cycle de vie d'une application, mais qui peut sembler intimidante pour les dÃ©butants.

**Ce guide vous apprendra :**
- ğŸ”„ Les diffÃ©rentes stratÃ©gies de migration
- ğŸ› ï¸ Les outils disponibles pour chaque type de migration
- ğŸ“ Comment planifier une migration
- ğŸ³ Comment utiliser Docker pour tester vos migrations
- âš ï¸ Les piÃ¨ges courants Ã  Ã©viter
- âœ… Comment valider vos migrations

**DurÃ©e estimÃ©e :** Lecture 30 minutes, mise en pratique variable selon complexitÃ©

---

## ğŸ¯ Pourquoi Migrer ?

### Raisons courantes de migration

| Raison | Exemple | FrÃ©quence |
|--------|---------|-----------|
| **Changement technologique** | MySQL â†’ PostgreSQL | TrÃ¨s courant |
| **MontÃ©e en charge** | SQLite â†’ PostgreSQL | Courant |
| **Ã‰volution architecture** | Monolithe SQL â†’ Microservices NoSQL | Courant |
| **CoÃ»ts** | Oracle â†’ MariaDB (open source) | Courant |
| **Performances** | MySQL â†’ MongoDB pour certains cas | Moyen |
| **FonctionnalitÃ©s spÃ©cifiques** | SQL classique â†’ Neo4j (graphes) | Moins courant |
| **Cloud migration** | On-premise â†’ Cloud (AWS RDS, etc.) | TrÃ¨s courant |

### ğŸ’¡ ScÃ©narios rÃ©els

**ScÃ©nario 1 : Startup qui grandit**
```
SQLite (prototype) â†’ PostgreSQL (production)
Raison : SQLite limitÃ© pour la concurrence
```

**ScÃ©nario 2 : Optimisation des coÃ»ts**
```
Oracle â†’ PostgreSQL
Raison : RÃ©duire les licences tout en gardant les fonctionnalitÃ©s
```

**ScÃ©nario 3 : Modernisation**
```
MySQL â†’ MongoDB
Raison : SchÃ©ma flexible pour donnÃ©es JSON
```

---

## ğŸ—ºï¸ Types de Migrations

### 1. Migrations HomogÃ¨nes (MÃªme Type)

**DÃ©finition :** Migration entre deux SGBD du mÃªme type (SQL â†’ SQL, NoSQL â†’ NoSQL)

**Exemples :**
- MySQL â†’ MariaDB
- PostgreSQL 12 â†’ PostgreSQL 15
- MongoDB 4.4 â†’ MongoDB 6.0

**DifficultÃ© :** ğŸŸ¢ Facile Ã  ğŸŸ¡ Moyenne

**Avantages :**
- âœ… Structure des donnÃ©es similaire
- âœ… Outils natifs souvent disponibles
- âœ… Moins de transformation de donnÃ©es

### 2. Migrations HÃ©tÃ©rogÃ¨nes (Type DiffÃ©rent)

**DÃ©finition :** Migration entre SGBD de types diffÃ©rents

**Exemples :**
- SQL â†’ NoSQL : MySQL â†’ MongoDB
- NoSQL â†’ SQL : MongoDB â†’ PostgreSQL
- Relationnel â†’ Graphe : PostgreSQL â†’ Neo4j
- SQL â†’ Time-Series : MySQL â†’ InfluxDB

**DifficultÃ© :** ğŸŸ¡ Moyenne Ã  ğŸ”´ Difficile

**DÃ©fis :**
- âš ï¸ ModÃ¨les de donnÃ©es trÃ¨s diffÃ©rents
- âš ï¸ Transformation complexe nÃ©cessaire
- âš ï¸ Perte potentielle de fonctionnalitÃ©s
- âš ï¸ NÃ©cessite souvent du code personnalisÃ©

---

## ğŸ“Š Matrice de CompatibilitÃ©

### FacilitÃ© de Migration

| Source â†“ / Cible â†’ | MySQL | PostgreSQL | MongoDB | Redis | Elasticsearch | Neo4j |
|-------------------|-------|------------|---------|-------|---------------|-------|
| **MySQL** | ğŸŸ¢ Trivial | ğŸŸ¡ Moyen | ğŸŸ¡ Moyen | ğŸ”´ Difficile | ğŸŸ¡ Moyen | ğŸ”´ Difficile |
| **PostgreSQL** | ğŸŸ¡ Moyen | ğŸŸ¢ Trivial | ğŸŸ¡ Moyen | ğŸ”´ Difficile | ğŸŸ¡ Moyen | ğŸ”´ Difficile |
| **MongoDB** | ğŸ”´ Difficile | ğŸ”´ Difficile | ğŸŸ¢ Trivial | ğŸŸ¡ Moyen | ğŸŸ¢ Facile | ğŸŸ¡ Moyen |
| **Redis** | ğŸ”´ Difficile | ğŸ”´ Difficile | ğŸŸ¡ Moyen | ğŸŸ¢ Trivial | ğŸŸ¡ Moyen | ğŸ”´ Difficile |
| **SQLite** | ğŸŸ¢ Facile | ğŸŸ¢ Facile | ğŸŸ¡ Moyen | ğŸ”´ Difficile | ğŸŸ¡ Moyen | ğŸ”´ Difficile |

**LÃ©gende :**
- ğŸŸ¢ Facile : Outils automatiques disponibles, peu de transformation
- ğŸŸ¡ Moyen : NÃ©cessite scripts, transformation modÃ©rÃ©e
- ğŸ”´ Difficile : Transformation complexe, code personnalisÃ© nÃ©cessaire

---

## ğŸ“ Concepts ClÃ©s

### 1. ETL (Extract, Transform, Load)

Le processus ETL est le cÅ“ur de toute migration de donnÃ©es.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   EXTRACT   â”‚â”€â”€â”€â”€â†’â”‚  TRANSFORM  â”‚â”€â”€â”€â”€â†’â”‚    LOAD     â”‚
â”‚ (Source DB) â”‚     â”‚ (Conversion)â”‚     â”‚ (Target DB) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**DÃ©tail des Ã©tapes :**

#### Extract (Extraction)
```bash
# Exemple : Exporter depuis MySQL
mysqldump -u root -p ma_base > data.sql

# Exemple : Exporter depuis MongoDB
mongoexport --db ma_base --collection users --out users.json
```

#### Transform (Transformation)
```python
# Exemple : Convertir des donnÃ©es
import json

# Lire depuis MongoDB (JSON)
with open('users.json') as f:
    users_mongo = [json.loads(line) for line in f]

# Transformer pour PostgreSQL (SQL)
sql_inserts = []
for user in users_mongo:
    sql = f"INSERT INTO users (id, name, email) VALUES ({user['_id']}, '{user['name']}', '{user['email']}');"
    sql_inserts.append(sql)

# Sauvegarder
with open('users.sql', 'w') as f:
    f.write('\n'.join(sql_inserts))
```

#### Load (Chargement)
```bash
# Charger dans PostgreSQL
psql -U postgres -d ma_base -f users.sql
```

### 2. SchÃ©ma vs Schemaless

**Bases relationnelles (SQL) - SchÃ©ma strict**
```sql
-- Structure dÃ©finie Ã  l'avance
CREATE TABLE users (
    id INT PRIMARY KEY,
    name VARCHAR(100) NOT NULL,
    email VARCHAR(255) UNIQUE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP
);
```

**Bases NoSQL - SchÃ©ma flexible**
```javascript
// Structure libre
{
    "_id": 1,
    "name": "Alice",
    "email": "alice@example.com",
    "preferences": {  // Peut varier d'un document Ã  l'autre
        "theme": "dark",
        "notifications": true
    }
}
```

**ğŸ’¡ Implication pour la migration :**
- SQL â†’ NoSQL : Facile (ajouter flexibilitÃ©)
- NoSQL â†’ SQL : Difficile (contraindre structure)

### 3. Types de DonnÃ©es

Chaque SGBD a ses propres types de donnÃ©es.

**Exemple de correspondance MySQL â†’ PostgreSQL :**

| MySQL | PostgreSQL | Notes |
|-------|------------|-------|
| `INT` | `INTEGER` | Identique |
| `VARCHAR(n)` | `VARCHAR(n)` | Identique |
| `TEXT` | `TEXT` | Identique |
| `DATETIME` | `TIMESTAMP` | Comportement diffÃ©rent avec fuseaux horaires |
| `TINYINT(1)` | `BOOLEAN` | MySQL utilise 0/1, PostgreSQL TRUE/FALSE |
| `ENUM('a','b')` | `TEXT` ou custom ENUM | PostgreSQL plus strict |

### 4. ClÃ©s et Contraintes

**Ã‰lÃ©ments Ã  migrer :**
- âœ… ClÃ©s primaires (PRIMARY KEY)
- âœ… ClÃ©s Ã©trangÃ¨res (FOREIGN KEY)
- âœ… Index
- âœ… Contraintes UNIQUE
- âœ… Contraintes CHECK
- âœ… Valeurs par dÃ©faut (DEFAULT)
- âœ… Triggers
- âœ… Vues
- âœ… ProcÃ©dures stockÃ©es

âš ï¸ **Attention :** La syntaxe varie entre SGBD !

---

## ğŸ› ï¸ Outils de Migration

### 1. Outils Natifs (Dumps)

Chaque SGBD fournit des outils pour exporter/importer.

#### MySQL / MariaDB

**Export :**
```bash
# Export complet
mysqldump -u root -p --all-databases > full_backup.sql

# Export d'une base spÃ©cifique
mysqldump -u root -p ma_base > ma_base.sql

# Export structure uniquement (sans donnÃ©es)
mysqldump -u root -p --no-data ma_base > structure.sql

# Export donnÃ©es uniquement (sans structure)
mysqldump -u root -p --no-create-info ma_base > donnees.sql
```

**Import :**
```bash
# Import
mysql -u root -p < full_backup.sql

# Ou dans le shell MySQL
mysql -u root -p
source /chemin/vers/fichier.sql;
```

#### PostgreSQL

**Export :**
```bash
# Export complet (toutes bases)
pg_dumpall -U postgres > full_backup.sql

# Export d'une base
pg_dump -U postgres ma_base > ma_base.sql

# Export en format personnalisÃ© (compressÃ©)
pg_dump -U postgres -Fc ma_base > ma_base.dump
```

**Import :**
```bash
# Import SQL
psql -U postgres ma_base < ma_base.sql

# Import format personnalisÃ©
pg_restore -U postgres -d ma_base ma_base.dump
```

#### MongoDB

**Export :**
```bash
# Export d'une base (format JSON)
mongodump --db ma_base --out /backup/

# Export d'une collection (JSON)
mongoexport --db ma_base --collection users --out users.json

# Export en CSV
mongoexport --db ma_base --collection users --type=csv --fields name,email --out users.csv
```

**Import :**
```bash
# Import d'une base
mongorestore --db ma_base /backup/ma_base/

# Import d'une collection
mongoimport --db ma_base --collection users --file users.json
```

#### Redis

**Export :**
```bash
# Redis sauvegarde automatiquement dans dump.rdb
# Forcer une sauvegarde
redis-cli SAVE

# Copier le fichier dump.rdb
cp /var/lib/redis/dump.rdb /backup/
```

**Import :**
```bash
# Restaurer : remplacer dump.rdb et redÃ©marrer Redis
cp /backup/dump.rdb /var/lib/redis/
redis-cli SHUTDOWN
redis-server
```

### 2. Outils Tiers

#### pgLoader (TrÃ¨s populaire pour migrations vers PostgreSQL)

**Installation :**
```bash
# Avec Docker (recommandÃ©)
docker pull dimitri/pgloader
```

**Migration MySQL â†’ PostgreSQL :**
```bash
# Commande simple
docker run --rm -it dimitri/pgloader \
    pgloader mysql://user:pass@mysql_host/db_source \
             postgresql://user:pass@pg_host/db_target

# Avec fichier de configuration (plus de contrÃ´le)
docker run --rm -it -v $(pwd):/data dimitri/pgloader \
    pgloader /data/migration.load
```

**Fichier de configuration `migration.load` :**
```lisp
LOAD DATABASE
    FROM mysql://root:password@mariadb_source:3306/ma_base
    INTO postgresql://postgres:password@postgres_target:5432/ma_base

WITH include drop, create tables, create indexes, reset sequences

SET maintenance_work_mem to '512MB',
    work_mem to '128MB'

CAST type datetime to timestamp
      drop default drop not null using zero-dates-to-null;
```

#### DBeaver (Interface graphique)

**Avantages :**
- âœ… Interface visuelle
- âœ… Support de nombreux SGBD
- âœ… Assistant de migration intÃ©grÃ©

**Utilisation :**
1. Connecter les deux bases (source + cible)
2. Clic droit sur table source â†’ "Export Data"
3. Choisir le format et la destination
4. Import dans la base cible

#### ETL Tools (Pour migrations complexes)

| Outil | Type | Usage | Gratuit |
|-------|------|-------|---------|
| **Apache NiFi** | ETL Visuel | Flux de donnÃ©es complexes | âœ… |
| **Talend** | ETL | Transformations avancÃ©es | âš ï¸ Community |
| **Pentaho** | ETL | Business Intelligence | âš ï¸ Community |
| **Airbyte** | ELT Moderne | Pipelines de donnÃ©es | âœ… |

### 3. Scripts PersonnalisÃ©s

Pour les migrations complexes, souvent nÃ©cessaire d'Ã©crire des scripts.

**Langages populaires :**
- ğŸ Python (pandas, SQLAlchemy)
- â˜• Java (JDBC)
- ğŸŸ¦ Node.js (Sequelize, Mongoose)
- ğŸ¹ Go (database/sql)

**Exemple Python (MySQL â†’ PostgreSQL) :**
```python
import pymysql
import psycopg2
from psycopg2 import sql

# Connexion source (MySQL)
mysql_conn = pymysql.connect(
    host='localhost',
    user='root',
    password='mysql_pass',
    database='source_db'
)

# Connexion cible (PostgreSQL)
pg_conn = psycopg2.connect(
    host='localhost',
    user='postgres',
    password='pg_pass',
    database='target_db'
)

# Extraction depuis MySQL
with mysql_conn.cursor() as cursor:
    cursor.execute("SELECT id, name, email FROM users")
    users = cursor.fetchall()

# Chargement dans PostgreSQL
with pg_conn.cursor() as cursor:
    for user in users:
        cursor.execute(
            "INSERT INTO users (id, name, email) VALUES (%s, %s, %s)",
            user
        )
    pg_conn.commit()

print(f"MigrÃ© {len(users)} utilisateurs")

# Fermeture
mysql_conn.close()
pg_conn.close()
```

---

## ğŸ“ Guide Pratique par Type de Migration

### Migration 1 : MySQL â†’ PostgreSQL

**DifficultÃ© :** ğŸŸ¡ Moyenne

**Ã‰tapes :**

#### MÃ©thode 1 : pgLoader (RecommandÃ©e)

```bash
# 1. PrÃ©parer l'environnement Docker
cat > docker-compose.yml << 'EOF'
version: '3.8'
services:
  mysql_source:
    image: mysql:8
    environment:
      MYSQL_ROOT_PASSWORD: mysql_pass
      MYSQL_DATABASE: source_db
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql

  postgres_target:
    image: postgres:15
    environment:
      POSTGRES_PASSWORD: pg_pass
      POSTGRES_DB: target_db
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

volumes:
  mysql_data:
  pg_data:
EOF

# 2. DÃ©marrer les bases
docker-compose up -d

# 3. InsÃ©rer des donnÃ©es de test dans MySQL
docker exec -it <mysql_container> mysql -u root -pmysql_pass -e "
USE source_db;
CREATE TABLE users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(100),
    email VARCHAR(255)
);
INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com');
"

# 4. CrÃ©er le fichier de migration
cat > migration.load << 'EOF'
LOAD DATABASE
    FROM mysql://root:mysql_pass@mysql_source:3306/source_db
    INTO postgresql://postgres:pg_pass@postgres_target:5432/target_db

WITH include drop, create tables
EOF

# 5. ExÃ©cuter la migration avec pgLoader
docker run --rm --network=host dimitri/pgloader \
    pgloader mysql://root:mysql_pass@localhost:3306/source_db \
             postgresql://postgres:pg_pass@localhost:5432/target_db

# 6. VÃ©rifier dans PostgreSQL
docker exec -it <postgres_container> psql -U postgres -d target_db -c "SELECT * FROM users;"
```

#### MÃ©thode 2 : Dump + Conversion manuelle

```bash
# 1. Export depuis MySQL
mysqldump -u root -p --compatible=postgresql source_db > mysql_dump.sql

# 2. Conversion manuelle (exemples de changements nÃ©cessaires)
# MySQL:  AUTO_INCREMENT -> PostgreSQL: SERIAL
# MySQL:  TINYINT(1) -> PostgreSQL: BOOLEAN
# MySQL:  ` (backticks) -> PostgreSQL: " (guillemets)

# Exemple de script sed pour conversions simples
sed -i 's/AUTO_INCREMENT/SERIAL/g' mysql_dump.sql
sed -i "s/\`/\"/g" mysql_dump.sql

# 3. Import dans PostgreSQL
psql -U postgres -d target_db -f mysql_dump.sql
```

**PiÃ¨ges courants :**
- âš ï¸ Les types DATETIME vs TIMESTAMP (gestion timezone)
- âš ï¸ Les quotes : MySQL utilise `, PostgreSQL utilise "
- âš ï¸ Les ENUM : Syntaxe diffÃ©rente
- âš ï¸ Les AUTO_INCREMENT vs SERIAL

### Migration 2 : MongoDB â†’ PostgreSQL

**DifficultÃ© :** ğŸ”´ Difficile (schemaless â†’ schema)

**StratÃ©gie :** Normaliser les documents JSON en tables relationnelles

**Exemple :**

**Document MongoDB (users) :**
```json
{
    "_id": ObjectId("..."),
    "name": "Alice",
    "email": "alice@example.com",
    "addresses": [
        {
            "type": "home",
            "street": "123 Main St",
            "city": "Paris"
        },
        {
            "type": "work",
            "street": "456 Work Ave",
            "city": "Lyon"
        }
    ],
    "created_at": ISODate("2024-01-15T10:30:00Z")
}
```

**SchÃ©ma PostgreSQL (normalisÃ©) :**
```sql
-- Table principale
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(255) UNIQUE,
    created_at TIMESTAMP
);

-- Table liÃ©e (relation 1-N)
CREATE TABLE addresses (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    type VARCHAR(20),
    street VARCHAR(255),
    city VARCHAR(100)
);
```

**Script Python de migration :**
```python
from pymongo import MongoClient
import psycopg2

# Connexion MongoDB
mongo_client = MongoClient('mongodb://localhost:27017/')
mongo_db = mongo_client['source_db']

# Connexion PostgreSQL
pg_conn = psycopg2.connect(
    host='localhost',
    user='postgres',
    password='password',
    database='target_db'
)
pg_cursor = pg_conn.cursor()

# Migration
for doc in mongo_db.users.find():
    # InsÃ©rer utilisateur
    pg_cursor.execute(
        "INSERT INTO users (name, email, created_at) VALUES (%s, %s, %s) RETURNING id",
        (doc['name'], doc['email'], doc['created_at'])
    )
    user_id = pg_cursor.fetchone()[0]

    # InsÃ©rer adresses liÃ©es
    for addr in doc.get('addresses', []):
        pg_cursor.execute(
            "INSERT INTO addresses (user_id, type, street, city) VALUES (%s, %s, %s, %s)",
            (user_id, addr['type'], addr['street'], addr['city'])
        )

pg_conn.commit()
pg_cursor.close()
pg_conn.close()
```

**DÃ©fis spÃ©cifiques :**
- ğŸ”„ Normalisation des donnÃ©es imbriquÃ©es
- ğŸ”„ Gestion des tableaux (arrays)
- ğŸ”„ Choix du schÃ©ma (normalisation vs JSONB)
- ğŸ”„ ObjectId MongoDB â†’ ID PostgreSQL

**Alternative : Utiliser JSONB**

Si vous voulez conserver la flexibilitÃ© :
```sql
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    data JSONB  -- Stocker le document complet
);

INSERT INTO users (data) VALUES ('{"name": "Alice", "email": "alice@example.com"}');

-- RequÃªte sur le JSON
SELECT data->>'name' AS name FROM users WHERE data->>'email' = 'alice@example.com';
```

### Migration 3 : PostgreSQL â†’ MongoDB

**DifficultÃ© :** ğŸŸ¡ Moyenne (schema â†’ schemaless)

**StratÃ©gie :** DÃ©normaliser les tables relationnelles en documents

**SchÃ©ma PostgreSQL :**
```sql
-- Tables normalisÃ©es
CREATE TABLE users (
    id SERIAL PRIMARY KEY,
    name VARCHAR(100),
    email VARCHAR(255)
);

CREATE TABLE orders (
    id SERIAL PRIMARY KEY,
    user_id INT REFERENCES users(id),
    product VARCHAR(100),
    amount DECIMAL(10,2),
    created_at TIMESTAMP
);
```

**Documents MongoDB (dÃ©normalisÃ©s) :**
```javascript
// Un seul document contenant tout
{
    "_id": 1,
    "name": "Alice",
    "email": "alice@example.com",
    "orders": [
        {
            "id": 101,
            "product": "Laptop",
            "amount": 999.99,
            "created_at": ISODate("2024-01-15")
        },
        {
            "id": 102,
            "product": "Mouse",
            "amount": 29.99,
            "created_at": ISODate("2024-01-16")
        }
    ]
}
```

**Script Python :**
```python
import psycopg2
from pymongo import MongoClient

# Connexion PostgreSQL
pg_conn = psycopg2.connect(
    host='localhost',
    user='postgres',
    password='password',
    database='source_db'
)

# Connexion MongoDB
mongo_client = MongoClient('mongodb://localhost:27017/')
mongo_db = mongo_client['target_db']

# Extraction et transformation
with pg_conn.cursor() as cursor:
    cursor.execute("SELECT id, name, email FROM users")
    users = cursor.fetchall()

    for user in users:
        user_id, name, email = user

        # RÃ©cupÃ©rer les commandes de l'utilisateur
        cursor.execute(
            "SELECT id, product, amount, created_at FROM orders WHERE user_id = %s",
            (user_id,)
        )
        orders = cursor.fetchall()

        # CrÃ©er le document MongoDB
        doc = {
            "_id": user_id,
            "name": name,
            "email": email,
            "orders": [
                {
                    "id": o[0],
                    "product": o[1],
                    "amount": float(o[2]),
                    "created_at": o[3]
                }
                for o in orders
            ]
        }

        # InsÃ©rer dans MongoDB
        mongo_db.users.insert_one(doc)

print("Migration terminÃ©e")
```

### Migration 4 : SQLite â†’ PostgreSQL

**DifficultÃ© :** ğŸŸ¢ Facile

**MÃ©thode recommandÃ©e : pgLoader**

```bash
# Commande directe
pgloader sqlite:///path/to/database.db postgresql://user:pass@localhost/dbname

# Ou via Docker
docker run --rm -v $(pwd):/data dimitri/pgloader \
    pgloader /data/database.db postgresql://user:pass@host/dbname
```

**MÃ©thode alternative : Dump + Import**

```bash
# 1. Exporter SQLite en SQL
sqlite3 database.db .dump > sqlite_dump.sql

# 2. Quelques ajustements (optionnels)
# SQLite utilise des types flexibles, PostgreSQL plus stricts
sed -i 's/AUTOINCREMENT/SERIAL/g' sqlite_dump.sql

# 3. CrÃ©er la base PostgreSQL
psql -U postgres -c "CREATE DATABASE target_db;"

# 4. Importer
psql -U postgres -d target_db -f sqlite_dump.sql
```

### Migration 5 : MySQL â†’ MongoDB

**DifficultÃ© :** ğŸŸ¡ Moyenne

**Script Node.js (avec Sequelize + Mongoose) :**

```javascript
const mysql = require('mysql2/promise');
const mongoose = require('mongoose');

// Connexion MySQL
const mysqlConn = await mysql.createConnection({
    host: 'localhost',
    user: 'root',
    password: 'password',
    database: 'source_db'
});

// Connexion MongoDB
await mongoose.connect('mongodb://localhost:27017/target_db');

// SchÃ©ma MongoDB
const UserSchema = new mongoose.Schema({
    name: String,
    email: String,
    created_at: Date
}, { _id: false, autoIndex: false });

const User = mongoose.model('User', UserSchema);

// Migration
const [rows] = await mysqlConn.execute('SELECT id, name, email, created_at FROM users');

for (const row of rows) {
    await User.create({
        _id: row.id,  // Utiliser l'ID MySQL comme _id MongoDB
        name: row.name,
        email: row.email,
        created_at: row.created_at
    });
}

console.log(`MigrÃ© ${rows.length} utilisateurs`);

// Fermeture
await mysqlConn.end();
await mongoose.disconnect();
```

---

## âœ… Validation Post-Migration

AprÃ¨s toute migration, il est **crucial** de valider l'intÃ©gritÃ© des donnÃ©es.

### Checklist de Validation

#### 1. Comptage des Enregistrements

```sql
-- Source (MySQL)
SELECT COUNT(*) FROM users;

-- Cible (PostgreSQL)
SELECT COUNT(*) FROM users;

-- Les deux doivent Ãªtre identiques !
```

#### 2. VÃ©rification des Valeurs

```sql
-- Comparer des Ã©chantillons alÃ©atoires
-- Source
SELECT * FROM users ORDER BY RAND() LIMIT 10;

-- Cible
SELECT * FROM users ORDER BY RANDOM() LIMIT 10;
```

#### 3. VÃ©rification des Contraintes

```sql
-- VÃ©rifier les clÃ©s primaires
SELECT COUNT(DISTINCT id) FROM users;  -- Doit Ãªtre Ã©gal au COUNT(*) total

-- VÃ©rifier les clÃ©s Ã©trangÃ¨res
SELECT COUNT(*) FROM orders WHERE user_id NOT IN (SELECT id FROM users);  -- Doit Ãªtre 0

-- VÃ©rifier les valeurs NULL
SELECT COUNT(*) FROM users WHERE email IS NULL;  -- Si email est NOT NULL
```

#### 4. VÃ©rification des Types de DonnÃ©es

```sql
-- PostgreSQL : VÃ©rifier les types
SELECT column_name, data_type
FROM information_schema.columns
WHERE table_name = 'users';
```

#### 5. Tests Fonctionnels

```python
# Exemple : Tester des requÃªtes typiques de l'application
import psycopg2

conn = psycopg2.connect(...)
cursor = conn.cursor()

# RequÃªte mÃ©tier 1 : Obtenir un utilisateur par email
cursor.execute("SELECT * FROM users WHERE email = %s", ('alice@example.com',))
user = cursor.fetchone()
assert user is not None, "Utilisateur non trouvÃ© aprÃ¨s migration"

# RequÃªte mÃ©tier 2 : Jointure
cursor.execute("""
    SELECT u.name, COUNT(o.id)
    FROM users u
    LEFT JOIN orders o ON u.id = o.user_id
    GROUP BY u.name
""")
results = cursor.fetchall()
assert len(results) > 0, "Aucun rÃ©sultat aprÃ¨s jointure"

print("âœ… Tous les tests passent")
```

### Script de Validation AutomatisÃ©

```python
import psycopg2
import pymysql

def validate_migration(mysql_config, pg_config):
    # Connexions
    mysql_conn = pymysql.connect(**mysql_config)
    pg_conn = psycopg2.connect(**pg_config)

    mysql_cursor = mysql_conn.cursor()
    pg_cursor = pg_conn.cursor()

    # Test 1 : Comptage
    mysql_cursor.execute("SELECT COUNT(*) FROM users")
    mysql_count = mysql_cursor.fetchone()[0]

    pg_cursor.execute("SELECT COUNT(*) FROM users")
    pg_count = pg_cursor.fetchone()[0]

    assert mysql_count == pg_count, f"Comptage diffÃ©rent : MySQL={mysql_count}, PG={pg_count}"
    print(f"âœ… Comptage OK : {mysql_count} enregistrements")

    # Test 2 : Ã‰chantillons
    mysql_cursor.execute("SELECT id, name, email FROM users ORDER BY id LIMIT 100")
    mysql_sample = set(mysql_cursor.fetchall())

    pg_cursor.execute("SELECT id, name, email FROM users ORDER BY id LIMIT 100")
    pg_sample = set(pg_cursor.fetchall())

    assert mysql_sample == pg_sample, "Ã‰chantillons diffÃ©rents"
    print("âœ… Ã‰chantillons identiques")

    # Test 3 : ClÃ©s Ã©trangÃ¨res
    pg_cursor.execute("""
        SELECT COUNT(*)
        FROM orders
        WHERE user_id NOT IN (SELECT id FROM users)
    """)
    orphaned = pg_cursor.fetchone()[0]
    assert orphaned == 0, f"{orphaned} enregistrements orphelins dÃ©tectÃ©s"
    print("âœ… IntÃ©gritÃ© rÃ©fÃ©rentielle OK")

    print("\nğŸ‰ Migration validÃ©e avec succÃ¨s")

    # Fermeture
    mysql_conn.close()
    pg_conn.close()

# Utilisation
validate_migration(
    mysql_config={'host': 'localhost', 'user': 'root', 'password': 'pass', 'database': 'source_db'},
    pg_config={'host': 'localhost', 'user': 'postgres', 'password': 'pass', 'database': 'target_db'}
)
```

---

## ğŸ¯ StratÃ©gies de Migration

### 1. Big Bang (Tout d'un coup)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Old DB     â”‚â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Active)   â”‚       â”‚  Migration complÃ¨te
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚  (weekend, nuit)
                      â†“
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  New DB     â”‚
                â”‚  (Active)   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Avantages :**
- âœ… Simple Ã  planifier
- âœ… Migration unique
- âœ… Pas de code de compatibilitÃ©

**InconvÃ©nients :**
- âŒ Downtime nÃ©cessaire
- âŒ Rollback difficile
- âŒ Risque Ã©levÃ©

**Quand utiliser :**
- Applications avec fenÃªtre de maintenance acceptable
- Migrations simples
- Petits volumes de donnÃ©es

### 2. Migration Progressive (Strangler Pattern)

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚  Application   â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                         â”‚
          â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
          â”‚              â”‚              â”‚
          â†“              â†“              â†“
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ Old DB  â”‚â”€â”€â”€â†’â”‚ Sync    â”‚â”€â”€â”€â†’â”‚ New DB  â”‚
    â”‚ (Read)  â”‚    â”‚ Process â”‚    â”‚ (Write) â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Phase 1: Dual write
    Phase 2: Dual read (new first)
    Phase 3: Old DB deprecated
```

**Avantages :**
- âœ… Pas de downtime
- âœ… Rollback facile
- âœ… Migration par Ã©tapes

**InconvÃ©nients :**
- âŒ Complexe Ã  implÃ©menter
- âŒ Code de compatibilitÃ© nÃ©cessaire
- âŒ Sync bidirectionnelle difficile

**Quand utiliser :**
- Applications critiques (pas de downtime)
- Migrations complexes
- Gros volumes

### 3. RÃ©plication Continue

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Old DB     â”‚â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  (Master)   â”‚           â”‚ Replication
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚ en continu
                          â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚  New DB     â”‚
                    â”‚  (Replica)  â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

    Ã€ J-day: Switch
```

**Outils :**
- MySQL â†’ PostgreSQL : pgLoader en mode suivi
- Avec Debezium (Change Data Capture)
- Avec Kafka Connect

**Avantages :**
- âœ… DonnÃ©es toujours Ã  jour
- âœ… FenÃªtre de migration courte
- âœ… Rollback possible

**InconvÃ©nients :**
- âŒ NÃ©cessite outils spÃ©cifiques
- âŒ Configuration complexe

---

## âš ï¸ PiÃ¨ges Courants et Solutions

### 1. Encodage de CaractÃ¨res

**ProblÃ¨me :**
```
MySQL (latin1) â†’ PostgreSQL (UTF-8)
RÃ©sultat : ï¿½ ï¿½ ï¿½ (caractÃ¨res corrompus)
```

**Solution :**
```bash
# VÃ©rifier l'encodage source
mysql -e "SHOW VARIABLES LIKE 'character_set%';"

# Convertir avant export
mysqldump --default-character-set=utf8mb4 ...

# SpÃ©cifier encodage dans PostgreSQL
psql -c "CREATE DATABASE target_db ENCODING 'UTF8';"
```

### 2. Pertes de DonnÃ©es

**ProblÃ¨me :**
```
Type source: BIGINT (jusqu'Ã  2^63)
Type cible: INT (jusqu'Ã  2^31)
RÃ©sultat : Overflow ou troncature
```

**Solution :**
```sql
-- Analyser les valeurs max avant migration
SELECT MAX(id) FROM big_table;  -- Si > 2^31, utiliser BIGINT

-- Adapter le schÃ©ma cible
CREATE TABLE big_table (
    id BIGINT PRIMARY KEY,  -- Pas INT
    ...
);
```

### 3. Contraintes Non MigrÃ©es

**ProblÃ¨me :**
```sql
-- MySQL : Contrainte implicite
email VARCHAR(255) UNIQUE

-- PostgreSQL : Pas d'index crÃ©Ã© automatiquement
```

**Solution :**
```sql
-- VÃ©rifier les index aprÃ¨s migration
SELECT * FROM pg_indexes WHERE tablename = 'users';

-- RecrÃ©er si nÃ©cessaire
CREATE UNIQUE INDEX idx_users_email ON users(email);
```

### 4. Performances DÃ©gradÃ©es

**ProblÃ¨me :**
RequÃªtes lentes aprÃ¨s migration

**Solutions :**
```sql
-- 1. Reconstruire les statistiques
ANALYZE;

-- 2. Reconstruire les index
REINDEX TABLE users;

-- 3. Vacuum (PostgreSQL)
VACUUM FULL users;

-- 4. VÃ©rifier les plans de requÃªtes
EXPLAIN ANALYZE SELECT * FROM users WHERE email = 'test@example.com';
```

### 5. Timezone et Dates

**ProblÃ¨me :**
```
MySQL DATETIME: 2024-01-15 10:30:00 (pas de timezone)
PostgreSQL TIMESTAMP: 2024-01-15 10:30:00+01:00 (avec timezone)
```

**Solution :**
```sql
-- SpÃ©cifier la timezone lors de la conversion
ALTER TABLE users ALTER COLUMN created_at
    TYPE TIMESTAMP WITH TIME ZONE
    USING created_at AT TIME ZONE 'Europe/Paris';
```

---

## ğŸ“‹ Checklist PrÃ©-Migration

Avant de dÃ©marrer une migration, vÃ©rifiez :

### PrÃ©paration

- [ ] **Backup complet** de la base source
- [ ] **Environnement de test** identique Ã  production
- [ ] **Plan de rollback** documentÃ©
- [ ] **Estimation du temps** de migration
- [ ] **FenÃªtre de maintenance** rÃ©servÃ©e (si big bang)
- [ ] **Outils installÃ©s** et testÃ©s
- [ ] **Ã‰quipe disponible** (et formÃ©e)

### Analyse de la Source

- [ ] **SchÃ©ma documentÃ©** (tables, relations, contraintes)
- [ ] **Volume de donnÃ©es** mesurÃ©
- [ ] **Types de donnÃ©es** inventoriÃ©s
- [ ] **Triggers/procÃ©dures** listÃ©s
- [ ] **Index et contraintes** recensÃ©s
- [ ] **RequÃªtes critiques** identifiÃ©es
- [ ] **Utilisateurs et permissions** documentÃ©s

### Configuration de la Cible

- [ ] **SchÃ©ma cible** dÃ©fini et validÃ©
- [ ] **CapacitÃ© suffisante** (disque, RAM)
- [ ] **RÃ©seau** configurÃ© (si migration distante)
- [ ] **Monitoring** en place
- [ ] **Logs** activÃ©s

### Tests

- [ ] **Migration test** rÃ©ussie (sur copie)
- [ ] **Validation** des donnÃ©es rÃ©ussie
- [ ] **Tests de performance** OK
- [ ] **Tests fonctionnels** de l'application OK

---

## ğŸ³ Utiliser Docker pour Tester les Migrations

Docker est **parfait** pour tester des migrations sans risque !

### Environnement de Test Complet

```yaml
version: '3.8'

services:
  # Source : MySQL
  mysql_source:
    image: mysql:8
    container_name: migration_source
    environment:
      MYSQL_ROOT_PASSWORD: source_password
      MYSQL_DATABASE: source_db
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./init_data.sql:/docker-entrypoint-initdb.d/init.sql  # DonnÃ©es de test

  # Cible : PostgreSQL
  postgres_target:
    image: postgres:15
    container_name: migration_target
    environment:
      POSTGRES_PASSWORD: target_password
      POSTGRES_DB: target_db
    ports:
      - "5432:5432"
    volumes:
      - pg_data:/var/lib/postgresql/data

  # Outil de migration : pgLoader
  pgloader:
    image: dimitri/pgloader
    container_name: migration_tool
    depends_on:
      - mysql_source
      - postgres_target
    volumes:
      - ./migration.load:/migration.load
    command: pgloader /migration.load

volumes:
  mysql_data:
  pg_data:
```

**Fichier `init_data.sql` (donnÃ©es de test) :**
```sql
CREATE TABLE users (
    id INT PRIMARY KEY AUTO_INCREMENT,
    name VARCHAR(100),
    email VARCHAR(255) UNIQUE,
    created_at DATETIME DEFAULT CURRENT_TIMESTAMP
);

INSERT INTO users (name, email) VALUES
    ('Alice', 'alice@example.com'),
    ('Bob', 'bob@example.com'),
    ('Charlie', 'charlie@example.com');
```

**Utilisation :**
```bash
# 1. DÃ©marrer l'environnement
docker-compose up -d

# 2. Attendre que tout soit prÃªt
sleep 10

# 3. Lancer la migration
docker-compose up pgloader

# 4. VÃ©rifier dans PostgreSQL
docker exec -it migration_target psql -U postgres -d target_db -c "SELECT * FROM users;"

# 5. Nettoyer et recommencer si besoin
docker-compose down -v
```

**Avantages :**
- âœ… Reproductible Ã  l'infini
- âœ… IsolÃ© de votre systÃ¨me
- âœ… Facile Ã  partager avec l'Ã©quipe
- âœ… Nettoyage propre

---

## ğŸ’¡ Bonnes Pratiques

### 1. Toujours Tester

```
âŒ Migration directe en production
âœ… Test â†’ Validation â†’ Production
```

**Workflow recommandÃ© :**
```
1. Backup source
2. Migration sur copie de test
3. Validation complÃ¨te
4. Correction des problÃ¨mes dÃ©tectÃ©s
5. Migration en production (avec plan B)
6. Validation post-migration
7. Monitoring intensif pendant 24-48h
```

### 2. Planifier le Rollback

**Avant la migration, prÃ©parez :**
- ğŸ“¦ Backup complet rÃ©cent
- ğŸ“ ProcÃ©dure de restauration documentÃ©e et testÃ©e
- â±ï¸ Temps de rollback estimÃ©
- ğŸ‘¥ Ã‰quipe disponible pour rollback d'urgence

**Conditions de rollback :**
```python
# DÃ©finir des seuils
if data_loss_percent > 1:
    trigger_rollback()

if performance_degradation > 50:
    trigger_rollback()

if critical_feature_broken:
    trigger_rollback()
```

### 3. Migration par Phases

Ne migrez pas tout d'un coup si possible.

**Exemple :**
```
Phase 1: Tables de rÃ©fÃ©rence (pays, catÃ©gories)
Phase 2: DonnÃ©es utilisateurs (non critiques)
Phase 3: DonnÃ©es transactionnelles
Phase 4: DonnÃ©es historiques (archives)
```

### 4. Monitoring et Logs

```bash
# Activer les logs dÃ©taillÃ©s
# PostgreSQL
ALTER SYSTEM SET log_min_duration_statement = 100;  # Log requÃªtes > 100ms

# MySQL
SET GLOBAL slow_query_log = 'ON';
SET GLOBAL long_query_time = 1;
```

**MÃ©triques Ã  surveiller :**
- ğŸ“Š Latence des requÃªtes
- ğŸ’¾ Utilisation disque
- ğŸ”¥ CPU et RAM
- ğŸ”— Nombre de connexions
- âš ï¸ Erreurs applicatives

### 5. Documentation

Documentez **tout** :
```markdown
# Migration MySQL â†’ PostgreSQL - 2024-11-15

## Contexte
- Raison : FonctionnalitÃ©s avancÃ©es de PostgreSQL nÃ©cessaires
- Base source : MySQL 8.0.35
- Base cible : PostgreSQL 15.4

## Environnement
- Source : mysql-prod-01 (192.168.1.10)
- Cible : postgres-prod-01 (192.168.1.20)

## DonnÃ©es
- Volume : 50 GB
- Tables : 45
- Enregistrements : ~10 millions

## Outils
- pgLoader 3.6.9
- Scripts Python personnalisÃ©s

## DurÃ©e
- EstimÃ©e : 2h
- RÃ©elle : 1h45m

## ProblÃ¨mes RencontrÃ©s
1. Type DATETIME â†’ TIMESTAMP (rÃ©solu en ajoutant timezone)
2. ClÃ© Ã©trangÃ¨re circulaire (rÃ©solu en dÃ©sactivant temporairement)

## Validation
- Comptage : OK (10,234,567 enregistrements)
- Tests fonctionnels : OK
- Performance : +15% plus rapide

## Ã‰quipe
- Lead : Alice (@alice)
- DBA : Bob (@bob)
- Dev : Charlie (@charlie)
```

---

## ğŸ“š Ressources ComplÃ©mentaires

### Outils RecommandÃ©s

| Outil | Usage | Lien |
|-------|-------|------|
| **pgLoader** | MySQL/SQLite â†’ PostgreSQL | [pgloader.io](https://pgloader.io) |
| **DBeaver** | Interface graphique universelle | [dbeaver.io](https://dbeaver.io) |
| **Flyway** | Gestion de versions de schÃ©ma | [flywaydb.org](https://flywaydb.org) |
| **Liquibase** | Gestion de versions de schÃ©ma | [liquibase.org](https://liquibase.org) |
| **Airbyte** | Pipelines de donnÃ©es modernes | [airbyte.com](https://airbyte.com) |
| **Debezium** | Change Data Capture | [debezium.io](https://debezium.io) |

### Documentation Officielle

- [pgLoader Documentation](https://pgloader.readthedocs.io/)
- [PostgreSQL Migration Guide](https://www.postgresql.org/docs/current/migration.html)
- [MongoDB Migration Tools](https://docs.mongodb.com/database-tools/)
- [AWS Database Migration Service](https://aws.amazon.com/dms/)

### Articles et Tutoriels

- [Migrating from MySQL to PostgreSQL](https://wiki.postgresql.org/wiki/Converting_from_other_Databases_to_PostgreSQL)
- [MongoDB to SQL Migration Strategies](https://www.mongodb.com/blog/post/migration-strategies)

---

## â“ FAQ - Questions FrÃ©quentes

**Q : Combien de temps prend une migration ?**
R : TrÃ¨s variable. Comptez :
- Petite base (< 1 GB) : 1-2 heures
- Moyenne (1-10 GB) : 2-8 heures
- Grande (> 10 GB) : 1-3 jours

**Q : Faut-il arrÃªter l'application pendant la migration ?**
R : DÃ©pend de la stratÃ©gie :
- Big Bang : Oui (downtime)
- Strangler Pattern : Non
- RÃ©plication continue : Courte fenÃªtre de switch

**Q : Comment migrer sans perte de donnÃ©es ?**
R :
1. Backup complet avant
2. Test sur environnement identique
3. Validation systÃ©matique
4. Monitoring post-migration

**Q : Peut-on annuler une migration ?**
R : Oui, si vous avez :
- Un backup rÃ©cent
- Un plan de rollback testÃ©
- DÃ©tectÃ© le problÃ¨me rapidement

**Q : MongoDB vers SQL : faut-il normaliser ?**
R : Pas toujours ! Options :
- Normalisation complÃ¨te (plusieurs tables)
- JSONB dans PostgreSQL (flexibilitÃ©)
- Hybride (tables + JSONB)

**Q : pgLoader fonctionne pour toutes les migrations SQL â†’ PostgreSQL ?**
R : Quasiment. Supporte :
- MySQL â†’ PostgreSQL (excellent)
- SQLite â†’ PostgreSQL (excellent)
- MS SQL â†’ PostgreSQL (bon)
- Fichiers CSV (excellent)

**Q : Comment gÃ©rer les trÃ¨s gros volumes (100+ GB) ?**
R :
- Migration par tables (parallÃ©lisation)
- Compression durant transfert
- Migration incrÃ©mentale
- Utiliser des outils spÃ©cialisÃ©s (AWS DMS, etc.)

**Q : Faut-il migrer les index ?**
R : Oui, mais :
- Migrer le schÃ©ma d'abord
- DÃ©sactiver les index durant import (vitesse)
- RecrÃ©er les index aprÃ¨s import

**Q : Comment tester les performances aprÃ¨s migration ?**
R : Comparer les mÃ©triques clÃ©s :
```sql
-- Temps d'exÃ©cution requÃªte critique
EXPLAIN ANALYZE SELECT ...;

-- Avant vs AprÃ¨s
```

**Q : Que faire si des donnÃ©es sont corrompues aprÃ¨s migration ?**
R :
1. Identifier l'Ã©tendue (combien d'enregistrements ?)
2. Comparer avec backup source
3. Migration ciblÃ©e des donnÃ©es corrompues
4. Ou rollback complet si trop de corruption

---

## âœ… RÃ©capitulatif

Vous avez appris :

- âœ… **Les types de migrations** : homogÃ¨nes vs hÃ©tÃ©rogÃ¨nes
- âœ… **Le processus ETL** : Extract, Transform, Load
- âœ… **Les outils** : pgLoader, DBeaver, scripts personnalisÃ©s
- âœ… **Les migrations courantes** : MySQLâ†’PostgreSQL, MongoDBâ†’SQL, etc.
- âœ… **La validation** : Comptages, Ã©chantillons, tests
- âœ… **Les stratÃ©gies** : Big Bang, Strangler, RÃ©plication
- âœ… **Les piÃ¨ges** : Encodage, types, contraintes, performances
- âœ… **Les bonnes pratiques** : Tester, documenter, planifier le rollback
- âœ… **Docker pour tester** : Environnement reproductible

**Points clÃ©s Ã  retenir :**
1. ğŸ§ª **Toujours tester** avant production
2. ğŸ’¾ **Backup obligatoire** avant toute migration
3. âœ… **Valider systÃ©matiquement** aprÃ¨s migration
4. ğŸ“ **Documenter** chaque Ã©tape
5. ğŸ³ **Utiliser Docker** pour tester sans risque

---

## ğŸš€ Aller Plus Loin

### Prochaines Ã©tapes suggÃ©rÃ©es

- **[Annexe A - RÃ©fÃ©rence des commandes](../annexes/A-reference-commandes.md)** - Commandes Docker et SQL utiles
- **[Annexe C - Gestion des volumes](../annexes/C-gestion-volumes.md)** - Backups et restaurations
- **[Cas pratique 04 - Env dev complet](04-env-dev-complet.md)** - Tester plusieurs BDD ensemble

### Pour approfondir

- Apprendre SQL avancÃ© (EXPLAIN, index, optimisation)
- Ã‰tudier les pipelines de donnÃ©es (Kafka, Airflow)
- Explorer le Change Data Capture (CDC) avec Debezium
- Se former sur les migrations Cloud (AWS DMS, Azure Database Migration)

---

ğŸ” Retour au [Sommaire](/SOMMAIRE.md)
